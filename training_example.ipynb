{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# from data.dataset import get_dataloaders_pca\n",
    "# from alexnet.augmentation.pca import ImageNetPCA\n",
    "\n",
    "# only take 10% of the dataset for PCA computation since using the full dataset will be statistically redundant\n",
    "# and computationally expensive\n",
    "# train_loader, _, _ = get_dataloaders_pca(sample_size=0.1, batch_size=512)\n",
    "# pca = ImageNetPCA(train_loader)\n",
    "# eigenvalues, eigenvectors = pca.fit()\n",
    "\n",
    "# print(\"Eigenvalues:\", eigenvalues)\n",
    "# print(\"Eigenvectors:\", eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = torch.tensor([0.0042, 0.0185, 0.1957])\n",
    "eigenvectors = torch.tensor([\n",
    "    [ 0.3957, -0.7228, -0.5666],\n",
    "    [-0.8135,  0.0105, -0.5815],\n",
    "    [ 0.4262,  0.6910, -0.5838],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddef63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet.base import AlexNetBase\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNetBase().to(device)\n",
    "\n",
    "# According to section 5 (Details of learning) of paper\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ab5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import get_dataloaders_training\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders_training(batch_size=128, sample_size=1)\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    # According to section 4.1, the noise should be drawn only once per epoch\n",
    "\n",
    "    with tqdm(total=len(train_loader.dataset), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\" images\") as pbar:\n",
    "        for batch in train_loader:\n",
    "            model.train()  # Set the model to training mode\n",
    "            images, labels = batch['pixel_values'], batch['labels']\n",
    "\n",
    "            # PCA Augmentation according to section 4.1 (Data Augmentation) of the paper\n",
    "            noise = torch.normal(0, 0.1, size=(train_loader.batch_size, 3))\n",
    "            eigen_noise = noise * eigenvalues # (batch_size, 3) * (3,)\n",
    "            principal_components = torch.mm(eigen_noise, eigenvectors.t()) # (batch_size, 3) @ (3, 3) = (batch_size, 3)\n",
    "            # Add the principal components to each pixel in the image\n",
    "            images = images + principal_components.view(train_loader.batch_size, 3, 1, 1)  # this will broadcast the principal components to match the image shape\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # zero the gradients, forward pass, compute loss, backward pass, and update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(images.size(0))\n",
    "\n",
    "    # perform validation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_loader.dataset) // val_loader.batch_size, desc=f\"Validation {epoch+1}/{epochs}\", unit=\" batches\") as pbar:\n",
    "            model.eval()\n",
    "            for batch in val_loader:\n",
    "                val_images, val_labels = batch['pixel_values'], batch['labels']\n",
    "                batch_size, num_crops = val_images.shape[:2]\n",
    "                val_images = val_images.view(-1, *val_images.shape[2:]) # (batch_size * num_crops, channels, height, width)\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "                val_outputs = model(val_images)\n",
    "                val_outputs = val_outputs.view(batch_size, num_crops, -1)\n",
    "                avg_outputs = val_outputs.mean(dim=1)\n",
    "                val_loss += criterion(avg_outputs, val_labels).item()\n",
    "            \n",
    "                pbar.update(1)\n",
    "\n",
    "    train_loss /= train_loader.batch_size\n",
    "    val_loss /= val_loader.batch_size\n",
    "    print(f\"Completed Epoch [{epoch+1}/{epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    lr_scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ff9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"alexnet_{epoch+1}_epochs.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
